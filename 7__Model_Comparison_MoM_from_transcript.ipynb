{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyadeep-Dey/AI-experiments/blob/main/7__Model_Comparison_MoM_from_transcript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai"
      ],
      "metadata": {
        "id": "WtUVraR_XgfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n"
      ],
      "metadata": {
        "id": "Yip5mfo-aiN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to write text into a file"
      ],
      "metadata": {
        "id": "aLmWPNE_X89q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_text_to_file(folder_path, file_name, write_text):\n",
        "\n",
        "  # Always mount Drive explicitly when using Google Drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  print(\"Drive mounted.\")\n",
        "\n",
        "  # Wait until MyDrive is available\n",
        "  mydrive_path = '/content/drive/MyDrive'\n",
        "  while not os.path.exists(mydrive_path):\n",
        "      print(\"Waiting for Drive to be ready...\")\n",
        "      time.sleep(1)\n",
        "\n",
        "  # Create folder path if it doesn't exist\n",
        "  folder_path = os.path.join(mydrive_path, folder_path)\n",
        "  os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "  # Define file path\n",
        "  file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "  # Write content to the file\n",
        "  with open(file_path, 'w') as file:\n",
        "      file.write(write_text)\n",
        "\n",
        "\n",
        "  print(\"File written successfully to:\", file_path)\n"
      ],
      "metadata": {
        "id": "k7yIwWlgLwSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try this out"
      ],
      "metadata": {
        "id": "IfCZlbe2OHBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writes to Google Drive\n",
        "write_text_to_file(\n",
        "    folder_path=\"Files/Text\",\n",
        "    file_name=\"example.txt\",\n",
        "    write_text=\"Hello, PLANET .. ! Howz life ? .... !!\"\n",
        ")"
      ],
      "metadata": {
        "id": "_w-hxNNRMzne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to read from a file"
      ],
      "metadata": {
        "id": "Ba0zTQuFShtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_from_file(folder_path, file_name):\n",
        "\n",
        "  # Always mount Drive explicitly when using Google Drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  print(\"Drive mounted.\")\n",
        "\n",
        "  # Wait until MyDrive is available\n",
        "  mydrive_path = '/content/drive/MyDrive'\n",
        "  while not os.path.exists(mydrive_path):\n",
        "      print(\"Waiting for Drive to be ready...\")\n",
        "      time.sleep(1)\n",
        "\n",
        "  # Path to the file\n",
        "  file_path = os.path.join(mydrive_path, folder_path, file_name)\n",
        "\n",
        "  # Check if the file exists\n",
        "  if os.path.exists(file_path):\n",
        "      # Read the content of the file\n",
        "      with open(file_path, 'r') as file:\n",
        "          contents = file.read()\n",
        "      return contents\n",
        "  else:\n",
        "      return \"File not found!\"\n"
      ],
      "metadata": {
        "id": "lxoY2_L3OOdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try this out"
      ],
      "metadata": {
        "id": "bMi2c5CBPIcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contents = read_text_from_file(\n",
        "    folder_path=\"Files/Text\",\n",
        "    file_name=\"example.txt\"\n",
        ")\n",
        "\n",
        "print(\"File contents:\")\n",
        "print(contents)"
      ],
      "metadata": {
        "id": "mYp1Tec3T3wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's start comparing models"
      ],
      "metadata": {
        "id": "wpsXUbnjX8Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "GPT_4o_mini = \"gpt-4o-mini\"\n",
        "GPT_4o =\"gpt-4o\"\n",
        "\n"
      ],
      "metadata": {
        "id": "GI6IOTwXP9Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "oMk5VfGxYB6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI using Secrets in Colab\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "VgVt8guSYGvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare 3 models for creating MoM\n",
        "We have already used \"whisper-1\" from Open AI to create a transcript from the audio recording . Now we'll create minutes of the meeting from this transcript using **gpt-4o-mini** , **gpt-4o** and **meta-llama/Meta-Llama-3.1-8B-Instruct** and compare the results."
      ],
      "metadata": {
        "id": "31KfW5h3CWHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read the transcript first\n",
        "\n",
        "contents = read_text_from_file(\n",
        "    folder_path=\"Files/Text\",\n",
        "    file_name=\"audio_file_name_202504181127.txt\"\n",
        ")\n",
        "\n",
        "#print(\"File contents:\")\n",
        "#display(contents) # using this instead of print so that we can see it in multi line"
      ],
      "metadata": {
        "id": "VUE7MQ4vDRYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create the system and user prompt which will be used for all 3 LLMs**"
      ],
      "metadata": {
        "id": "fXgY4xBBFVJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that produces minutes of meetings from transcripts,with summary, key discussion points, takeaways and \\\n",
        "action items with owners, in markdown.\"\n",
        "\n",
        "user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with date \\\n",
        ", location and attendees;discussion points; list of key government web-sites and initiatives; takeaways; and action items with summary \\\n",
        ", owners and details.Ensure all action items are captured. It's ok if you have extra action items . \\\n",
        "Use bullet points when required and ensure that there is no line space between a point and it's children or sub bullet points. \\\n",
        "Here is the transscript : \\ {contents}\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]\n",
        "\n",
        "#display(messages)"
      ],
      "metadata": {
        "id": "EDDOhjzZE9kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Call Open AI to generate Minutes of the Meeting\n"
      ],
      "metadata": {
        "id": "eWuQGnYGE52K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets try with mini\n",
        "completion = openai.chat.completions.create(\n",
        "        model=GPT_4o_mini,\n",
        "        messages=messages,\n",
        "    )\n",
        "meeting_minutes = completion.choices[0].message.content\n",
        "display(Markdown(meeting_minutes))\n",
        "\n",
        "# Writes to Google Drive\n",
        "write_text_to_file(\n",
        "    folder_path=\"Files/Text\",\n",
        "    file_name=\"gpt-4o-mini MoM.md\",\n",
        "    write_text=meeting_minutes\n",
        ")"
      ],
      "metadata": {
        "id": "PEkG8hwFFE2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and then OpenAI's newest GPT-4 variant\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "        model=GPT_4o,\n",
        "        messages=messages,\n",
        "    )\n",
        "meeting_minutes = completion.choices[0].message.content\n",
        "display(Markdown(meeting_minutes))\n",
        "\n",
        "# Writes to Google Drive\n",
        "write_text_to_file(\n",
        "    folder_path=\"Files/Text\",\n",
        "    file_name=\"gpt-4o MoM.md\",\n",
        "    write_text=meeting_minutes\n",
        ")"
      ],
      "metadata": {
        "id": "_eMTJUiBajam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call Llama to generate Minutes of the Meeting"
      ],
      "metadata": {
        "id": "NheGJL3rJRQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "3QRC2chtIjTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "#adds a padding token ([PAD]) to the tokenizerâ€™s vocabulary if it doesn't already have one.\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", padding=True).to(\"cuda\") # cuda -> use GPU\n",
        "#return_tensors=\"pt\" means it will return PyTorch tensors rather than a Python string or list.\n",
        "# The inputs is now a tensor, not a dictionary.\n",
        "\n",
        "# Access the input_ids and attention_mask directly as attributes\n",
        "input_ids = inputs  # or inputs.input_ids if the model expects it as a separate key\n",
        "# Create attention mask from input_ids - assuming padding token is 0\n",
        "attention_mask = (input_ids != tokenizer.pad_token_id).type(torch.int64).to(\"cuda\") # changed to create attention_mask from input_ids\n"
      ],
      "metadata": {
        "id": "nr8WRhdVJZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    attention_mask=attention_mask,\n",
        "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "    max_new_tokens=2000\n",
        ")\n"
      ],
      "metadata": {
        "id": "gWQWuywJKA-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(outputs[0])\n",
        "\n",
        "# we don't need to see prompt sent to LLM . Only response. Hence we split ..\n",
        "split_response = response.split(\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")\n",
        "meeting_minutes = split_response[1] # and take 2nd part = part after prompt\n",
        "\n",
        "\n",
        "display(Markdown(meeting_minutes))\n",
        "\n",
        "# Writes to Google Drive\n",
        "write_text_to_file(\n",
        "    folder_path=\"Files/Text\",\n",
        "    file_name=\"Llama MoM.md\",\n",
        "    write_text=meeting_minutes\n",
        ")"
      ],
      "metadata": {
        "id": "oRIEf734KaJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z8JXbeqUMqTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}