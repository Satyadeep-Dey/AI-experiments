{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyadeep-Dey/AI-experiments/blob/main/6_Quantization_%2B_Low_level_API_%2B_Call_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low Level APIs\n",
        "\n",
        "Hugging Face **low-level APIs** refer to the more granular, flexible building blocks provided by the transformers library that allow you to interact directly with models and tokenizers — without relying on high-level abstraction layers like pipeline.\n",
        "\n",
        "In this Notebook we look at the low level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
        "\n"
      ],
      "metadata": {
        "id": "aKs1PM-O-VQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Needed Libraries\n",
        "\n",
        "1. **requests** :\n",
        "    * **Purpose:** Simple HTTP library for making API requests.\n",
        "    * **Use case:** Useful for downloading models, datasets, or interacting with REST APIs.\n",
        "\n",
        "2. **torch**\n",
        "    * **Purpose:** PyTorch, a deep learning framework developed by Facebook.\n",
        "    * **Use case:** For building, training, and running neural networks.\n",
        "\n",
        "3. **bitsandbytes**\n",
        "    * **Purpose:** A lightweight CUDA extension for 8-bit and 4-bit optimizers and matrix multiplication.\n",
        "    * **Use case:** Used to reduce memory usage and speed up large models, especially in inference or fine-tuning. Commonly used with Hugging Face models.\n",
        "\n",
        "4. **transformers**\n",
        "    * **Purpose:** Hugging Face's Transformers library.\n",
        "    * **Use case:** Provides pre-trained transformer models like BERT, GPT, T5, etc., with easy APIs for text generation, classification, etc.\n",
        "\n",
        "5. **sentencepiece**\n",
        "    * **Purpose:** Tokenizer developed by Google for unsupervised text tokenization.\n",
        "    * **Use case:** Many Hugging Face models (e.g., T5, mBART) use it for handling subword units.\n",
        "\n",
        "6. **accelerate**\n",
        "    * **Purpose:** Another Hugging Face library for optimizing model training and inference.\n",
        "    * **Use case:** Helps scale training across CPUs, GPUs, or even TPUs with minimal code changes. Works great in multi-GPU setups too."
      ],
      "metadata": {
        "id": "crtEo12UIC6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does -q do in pip install command?\n",
        "       -q means quiet mode, so it suppresses the usual output during installation—keeps the notebook or terminal cleaner."
      ],
      "metadata": {
        "id": "StzHfjksKTQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate"
      ],
      "metadata": {
        "id": "NthhKJRwX1iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9zvDGWD5pKp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "xd7cEDUC6Lkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instruct models\n",
        "\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"   # Meta\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"         # Microsoft\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"                   # Google\n",
        "QWEN2 = \"Qwen/Qwen2-7B-Instruct\"                  # Alibaba\n",
        "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  # Mistral AI -> If this doesn't fit it your GPU memory, try others from the hub"
      ],
      "metadata": {
        "id": "UtN7OKILQato"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of doctors\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "KgxCLBJIT5Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization\n",
        "\n",
        "In machine learning — especially deep learning — quantization is the process of reducing the precision of the numbers used to represent a model's parameters (like weights and activations).\n",
        "\n",
        "Normally, models are trained and stored using 32-bit floating-point numbers (float32).\n",
        "Quantization reduces this to lower precision types like:\n",
        "  1. 16-bit (e.g., float16, bfloat16)\n",
        "  2. 8-bit (e.g., int8)\n",
        "  3. 4-bit (e.g., nf4, fp4)\n",
        "\n",
        "**Why Quantize?**\n",
        "Quantization is mainly used for efficiency:\n",
        "\n",
        "  * Lower Memory\t- Model takes up less RAM/VRAM\n",
        "  * Faster Inference\t- Smaller numbers → faster computation on some hardware\n",
        "  * Lower Power - Especially useful for edge devices (phones, Raspberry Pi, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "OT6QqyIeeL6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "hhOgL1p_T6-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's see what these parameters mean\n",
        "\n",
        "  * load_in_4bit=True.\n",
        "    * This enables 4-bit quantization, a very memory-efficient format.\n",
        "The model weights will be stored and loaded in 4-bit precision instead of 16-bit or 32-bit, drastically reducing memory usage.\n",
        "\n",
        "  * bnb_4bit_use_double_quant=True\n",
        "    * Double quantization is a technique to compress the model even further.\n",
        "It applies an additional quantization step on the quantization constants (i.e., quantizing the quantization parameters themselves).This helps improve compression with a minimal impact on accuracy.\n",
        "\n",
        "  * bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    * This specifies the data type for computation, which here is bfloat16 (Brain Floating Point 16).While weights are stored in 4-bit, operations are done in bfloat16, which is more precise and well supported by modern hardware (especially GPUs like A100, H100, etc.).\n",
        "\n",
        "  * bnb_4bit_quant_type=\"nf4\"\n",
        "    * This sets the quantization scheme to \"nf4\", which stands for Normalized Float 4.nf4 is a specialized 4-bit quantization method shown to perform better than traditional 4-bit formats.It maintains more dynamic range and accuracy compared to other 4-bit schemes like fp4.\n",
        "\n",
        "Summary:\n",
        "  * This config is telling your model to:\n",
        "    * Load in a very memory-efficient 4-bit format (nf4)\n",
        "    * Use double quantization to further shrink size\n",
        "    * Perform computations in bfloat16, a fast and reasonably accurate format on supported hardware\n",
        "    * This setup is often used to fine-tune or run large language models (like LLaMA or Mistral) on limited hardware (like consumer GPUs or smaller cloud instances)."
      ],
      "metadata": {
        "id": "me6OyDC-fDQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "#adds a padding token ([PAD]) to the tokenizer’s vocabulary if it doesn't already have one.\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", padding=True).to(\"cuda\") # cuda -> use GPU\n",
        "#return_tensors=\"pt\" means it will return PyTorch tensors rather than a Python string or list.\n",
        "# The inputs is now a tensor, not a dictionary.\n",
        "\n",
        "# Access the input_ids and attention_mask directly as attributes\n",
        "input_ids = inputs  # or inputs.input_ids if the model expects it as a separate key\n",
        "# Create attention mask from input_ids - assuming padding token is 0\n",
        "attention_mask = (input_ids != tokenizer.pad_token_id).type(torch.int64).to(\"cuda\") # changed to create attention_mask from input_ids\n"
      ],
      "metadata": {
        "id": "Zi8YXiwJHF59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "\n",
        "# What device_map=\"auto\" does:\n",
        "# device_map=\"auto\" automatically splits the model across all available GPUs (or just one if you only have one).\n",
        "# It’s especially useful for very large models like LLaMA, which might not fit on a single GPU.\n",
        "# Under the hood, it uses accelerate to analyze your available hardware and figure out the best layer-to-device mapping."
      ],
      "metadata": {
        "id": "S5jly421tno3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory footprint: {memory:,.1f} MB\")"
      ],
      "metadata": {
        "id": "bdbYaT8hWXWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    attention_mask=attention_mask,\n",
        "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "    max_new_tokens=80\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "SkYEXzbotcud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up\n",
        "\n",
        "del inputs, outputs, model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "2oL0RWU2ttZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's make a function and enable streaming\n",
        "\n",
        "Use a HuggingFace utility called TextStreamer so that results stream back.\n",
        "To stream results, we simply replace:  \n",
        "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
        "With:  \n",
        "`streamer = TextStreamer(tokenizer)`  \n",
        "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
        "\n",
        "also , added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
        "\n"
      ],
      "metadata": {
        "id": "iDCeJ20e4Hxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
        "\n",
        "def generate(model, messages):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  #tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\",padding=True, add_generation_prompt=True).to(\"cuda\")\n",
        "  # Access the input_ids and attention_mask directly as attributes\n",
        "  input_ids = inputs  # or inputs.input_ids if the model expects it as a separate key\n",
        "  # Create attention mask from input_ids - assuming padding token is 0\n",
        "  attention_mask = (input_ids != tokenizer.pad_token_id).type(torch.int64).to(\"cuda\") # changed to create attention_mask from input_ids\n",
        "\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "\n",
        "  # outputs = model.generate\n",
        "  #     (inputs, max_new_tokens=80, streamer=streamer)\n",
        "\n",
        "  outputs = model.generate(\n",
        "    inputs,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=80,\n",
        "    # pad_token_id=tokenizer.pad_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "    streamer=streamer\n",
        ")\n",
        "\n",
        "# Clean up\n",
        "  del tokenizer, streamer, model, inputs, outputs\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RO_VYZ3DZ7cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's now call some LLM"
      ],
      "metadata": {
        "id": "zYASIDHCsciC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(PHI3, messages)"
      ],
      "metadata": {
        "id": "RFjaY4Pdvbfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(LLAMA, messages) # Meta"
      ],
      "metadata": {
        "id": "1TUPfc03mDEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(QWEN2,messages) # Microsoft"
      ],
      "metadata": {
        "id": "-vyMGN63mTxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try another prompt now that model has been loaded\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me something about Assam\"}\n",
        "  ]\n",
        "generate(QWEN2,messages) # Microsoft"
      ],
      "metadata": {
        "id": "Uvpxnu-Y4RrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma from Google requires us to accept their terms in Hugging Face.**\n",
        "\n",
        "  * Visit this page to ask for access -\n",
        "    https://huggingface.co/google/gemma-2-2b-it"
      ],
      "metadata": {
        "id": "LqIOf6TXtPSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_gemma = [{\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Doctors\"}]\n",
        "# since Gemma from Google does not support system role\n",
        "generate(GEMMA2, message_gemma)"
      ],
      "metadata": {
        "id": "q1JW41D-viGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mixtral also requires us to accept their terms in Huggging Face .**\n",
        "\n",
        "  * Visit https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1 to ask for access.\n",
        "  * This requires a lot of CPU/GPU/memory !!"
      ],
      "metadata": {
        "id": "lVb1Mf99oAo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate(MIXTRAL,messages)\n",
        "# don't execute sice it runs out of disk space . I had 112.6 GB !!"
      ],
      "metadata": {
        "id": "0m8yjMB3ZTp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyQyYaFTn5V0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}