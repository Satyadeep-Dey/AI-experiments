{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyadeep-Dey/AI-experiments/blob/main/9__Anonymize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai requests"
      ],
      "metadata": {
        "id": "WtUVraR_XgfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import login\n",
        "import re # For using regular expressions (pattern matching)\n",
        "import json # For parsing JSON data from strings\n",
        "\n"
      ],
      "metadata": {
        "id": "Yip5mfo-aiN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function 1 : Write text into a file"
      ],
      "metadata": {
        "id": "aLmWPNE_X89q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_text_to_file(folder_path, file_name, write_text):\n",
        "\n",
        "  # Always mount Drive explicitly when using Google Drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  print(\"Drive mounted.\")\n",
        "\n",
        "  # Wait until MyDrive is available\n",
        "  mydrive_path = '/content/drive/MyDrive'\n",
        "  while not os.path.exists(mydrive_path):\n",
        "      print(\"Waiting for Drive to be ready...\")\n",
        "      time.sleep(1)\n",
        "\n",
        "  # Create folder path if it doesn't exist\n",
        "  folder_path = os.path.join(mydrive_path, folder_path)\n",
        "  os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "  # Define file path\n",
        "  file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "  # Write content to the file\n",
        "  with open(file_path, 'w') as file:\n",
        "      file.write(write_text)\n",
        "\n",
        "\n",
        "  print(\"File written successfully to:\", file_path)\n"
      ],
      "metadata": {
        "id": "k7yIwWlgLwSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function 2: Read from a file"
      ],
      "metadata": {
        "id": "Ba0zTQuFShtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_from_file(folder_path, file_name):\n",
        "\n",
        "  # Always mount Drive explicitly when using Google Drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  print(\"Drive mounted.\")\n",
        "\n",
        "  # Wait until MyDrive is available\n",
        "  mydrive_path = '/content/drive/MyDrive'\n",
        "  while not os.path.exists(mydrive_path):\n",
        "      print(\"Waiting for Drive to be ready...\")\n",
        "      time.sleep(1)\n",
        "\n",
        "  # Path to the file\n",
        "  file_path = os.path.join(mydrive_path, folder_path, file_name)\n",
        "\n",
        "  # Check if the file exists\n",
        "  if os.path.exists(file_path):\n",
        "      # Read the content of the file\n",
        "      with open(file_path, 'r') as file:\n",
        "          contents = file.read()\n",
        "      return contents\n",
        "  else:\n",
        "      return \"File not found!\"\n"
      ],
      "metadata": {
        "id": "lxoY2_L3OOdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "GPT_4o_mini = \"gpt-4o-mini\"\n",
        "GPT_4o =\"gpt-4o\"\n"
      ],
      "metadata": {
        "id": "GI6IOTwXP9Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "oMk5VfGxYB6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI using Secrets in Colab\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "VgVt8guSYGvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read the text first\n",
        "\n",
        "original_content = read_text_from_file(\n",
        "    folder_path=\"Files/Knowledge-Base\",\n",
        "    file_name=\"A Tale of Two Cities.txt\"\n",
        ")\n",
        "\n",
        "print(f\"The number of characters are : {len(original_content)}\")\n",
        "number_of_words = len(original_content.split())\n",
        "# Divides a string into a list of substrings based on a specified separator (default is whitespace) and then counts length of list\n",
        "print(f\"Number of words is : {number_of_words}\")\n",
        "print()\n",
        "#print(original_content)\n",
        "\n"
      ],
      "metadata": {
        "id": "VUE7MQ4vDRYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 1 : Entire Anomymization is done using LLM - Chat GPT from Open AI\n",
        "\n",
        "\n",
        "\n",
        "*   gpt-4o-mini is not good at this.\n",
        "*   gpt-4o does a good job=> cost is less than 10 cents for about 6000 words (input + output) . Takes about a minute.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FFDkXd-3l682"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant who receives some text and then replaces the names of people with new names \\\n",
        "keeping in mind the gender and nationality of the person in this text. \\\n",
        "Also change the name of the story and it's author.Do not put the changed text inside '**' \"\n",
        "\n",
        "user_prompt = \"Here is the text : \" + original_content\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]\n",
        "\n",
        "\n",
        "# print(f\"The number of characters are : {len(user_prompt)}\")\n",
        "# number_of_words = len(user_prompt.split())\n",
        "# # Divides a string into a list of substrings based on a specified separator (default is whitespace) and then counts length of list\n",
        "# print(f\"Number of words is : {number_of_words}\")\n"
      ],
      "metadata": {
        "id": "VuTUu77NmgcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets try with GPT_4o because Mini was terrible !\n",
        "completion = openai.chat.completions.create(\n",
        "        model=GPT_4o,\n",
        "        messages=messages,\n",
        "        temperature= 0.5\n",
        "    )\n"
      ],
      "metadata": {
        "id": "FpJlonkym4qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anon_data = completion.choices[0].message.content\n",
        "\n",
        "print(f\"The number of characters are : {len(anon_data)}\")\n",
        "number_of_words = len(anon_data.split())\n",
        "# Divides a string into a list of substrings based on a specified separator (default is whitespace) and then counts length of list\n",
        "print(f\"Number of words is : {number_of_words}\")\n",
        "\n",
        "#print(anon_data)\n"
      ],
      "metadata": {
        "id": "uyD4dYO_m9ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_text_to_file(\"Files/Knowledge-Base\", \"Anonymized by OpenAI_TOTC_V4.txt\", anon_data)"
      ],
      "metadata": {
        "id": "4YdQ6kJvsjMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 2 : Use OpenAI to do Named Entity recognition (NER) then anonymyze programatically by replacing names\n"
      ],
      "metadata": {
        "id": "eWuQGnYGE52K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 : Use OpenAI to do Named Entity recognition (NER) as Python LIST"
      ],
      "metadata": {
        "id": "iur5Ya-ZuuQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that does named entity recognition (NER) based on a text in user prompt. \\\n",
        "You will return data as the following python lists \\\n",
        "people_name = {} example : people_name ={'John Doe','Johnny Depp','Steffi Graf','Priya Das'}\\\n",
        "city_name ={} example : city_name ={'New Delhi','New York','Bangalore'}\\\n",
        "book_title = {} example : book_title = {'Kidnapped','Ramayana','Three Musketeers'}.\"\n",
        "\n",
        "user_prompt = \"Here is the text : \" + original_content\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "Z_EuqkIwB4LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets try with mini\n",
        "# seed = 42 -> to make this deterministic\n",
        "completion = openai.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=messages,\n",
        "        seed=42\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PEkG8hwFFE2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_py_list = completion.choices[0].message.content\n",
        "print(data_py_list)\n",
        "#write_text_to_file(\"Files/Knowledge-Base\", \"Python LIST _TOTC.txt\", data_py_list)"
      ],
      "metadata": {
        "id": "JB3top7xcC7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 : Use OpenAI to do Named Entity recognition (NER) as JSON"
      ],
      "metadata": {
        "id": "thFXBP1ReX7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we explain the JSON format so that we can program against the different parts and those parts don't change everytime\n",
        "system_message = \"\"\"You are an assistant that does named entity recognition (NER) based on a text in user prompt. \\\n",
        "You will identify the name of each and every person and every town and city .\\\n",
        "You will return data as the following JSON format \\\n",
        "{\n",
        "  'entities': {\n",
        "    'novel': {\n",
        "      'title': 'Three Musketeers',\n",
        "      'author': 'Alexander Dumas'\n",
        "    },\n",
        "    'settings': [\n",
        "      {\n",
        "        'location': 'New York',\n",
        "        'time_period': 'before and during the first world war'\n",
        "      },\n",
        "      {\n",
        "        'location': 'Bombay',\n",
        "        'time_period': 'before and during the second world war'\n",
        "      }\n",
        "    ],\n",
        "    'characters': [\n",
        "      {\n",
        "        'name': 'John Doe',\n",
        "        'description': 'A nice guy.'\n",
        "      },\n",
        "      {\n",
        "        'name': 'Steffi Graf',\n",
        "        'description': 'A great tennis player.'\n",
        "      }\n",
        "    ],\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"Here is the text : \" + original_content\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "_8qiDGrLb3gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# completion = openai.chat.completions.create(\n",
        "#         model=GPT_4o,\n",
        "#         messages=messages\n",
        "#     )\n",
        "\n",
        "#lets try with mini\n",
        "completion = openai.chat.completions.create(\n",
        "        model=GPT_4o_mini,\n",
        "        messages=messages,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "# completeness of output varies depending on prompt. Sometimes some minor characters and places are missed."
      ],
      "metadata": {
        "id": "pZUatgqLb9Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_json_format = completion.choices[0].message.content\n",
        "print(data_json_format)\n",
        "#write_text_to_file(\"Files/Knowledge-Base\", \"JSON _TOTC_V4.json\", data_json_format)"
      ],
      "metadata": {
        "id": "YWSLA-VdcAGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse and extract relevant data from JSON\n",
        "\n",
        "#data = data_json_format\n",
        "\n",
        "'''\n",
        "- `re.search(...)` — Searches the string `data` for the **first occurrence** of the pattern.\n",
        "- `r\"```json(.*?)```\"` — This is the raw regex pattern:\n",
        "  - ``` ```json ``` — Matches the opening marker (exactly three backticks followed by `json`).\n",
        "  - `(.*?)` — A **non-greedy capture group** that grabs everything in between.\n",
        "  - ``` ``` — Matches the closing triple backticks.\n",
        "- `re.DOTALL` — Tells the regex engine to treat **newline characters (`\\n`) as normal characters**,\n",
        "                so it can match JSON that spans **multiple lines**.\n",
        "\n",
        "'''\n",
        "match = re.search(r\"```json(.*?)```\", data_json_format, re.DOTALL)\n",
        "if match: #Checks whether the regular expression successfully found a JSON block between triple backticks in data\n",
        "    json_str = match.group(1).strip()\n",
        "    '''\n",
        "    Extracts the captured JSON content\n",
        "    match.group(1) — returns only the content captured in the first set of parentheses .\n",
        "    .strip() — removes any leading/trailing whitespace or newlines.\n",
        "    '''\n",
        "else:\n",
        "    # fallback: maybe data itself is JSON\n",
        "    json_str = data_json_format.strip() #Assigns the whole content of data, trimmed of leading/trailing whitespace,\n",
        "\n",
        "#print(json_str)\n",
        "\n",
        "try:\n",
        "    json_obj = json.loads(json_str)\n",
        "    entities = json_obj[\"entities\"]\n",
        "    novel = entities[\"novel\"]\n",
        "    title = novel[\"title\"]\n",
        "    author = novel[\"author\"]\n",
        "    # print(title)\n",
        "    # print(author)\n",
        "\n",
        "    other_names = [] # a list of other names\n",
        "    people_names = [] # # a list of people names\n",
        "    other_names.append(title)\n",
        "\n",
        "    settings = entities[\"settings\"] # this is a LIST of DICTIONARIES\n",
        "\n",
        "    for setting in settings:\n",
        "        #print(setting) -> #{'location': 'London', 'time_period': 'before and during the French Revolution'}\n",
        "        other_names.append(setting[\"location\"]) # Extract location from this dictionary and append to LIST other_names\n",
        "\n",
        "    characters = entities[\"characters\"] # this is a LIST of DICTIONARIES\n",
        "\n",
        "    for character in characters:\n",
        "        people_names.append(character['name']) # Extract name from this dictionary and add to LIST people_names\n",
        "\n",
        "    people_names.append(author) # also add name of author since this needs to be anonymized as well\n",
        "\n",
        "    # IMPORTANT : we can also create a LIST of people_names from characters LIST directly as below\n",
        "    #people_names = [character[\"name\"] for character in characters]  # create a LIST of names\n",
        "\n",
        "    for people_name in people_names : # print names one by one\n",
        "        print(people_name)\n",
        "\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    for other in  other_names: # print name of novel , cities etc.\n",
        "        print(other)\n",
        "\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"JSON parsing error:\", e)\n",
        "    print(\"Raw data:\", json_str)\n"
      ],
      "metadata": {
        "id": "q63D7kyXeFiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note :\n",
        "You can now write some logic in python to replace these names with other names to anomymyze people, city and book name .\n",
        "But making it generic is a challenge and probably not worth it given that Open AI has already done the anonymization for you."
      ],
      "metadata": {
        "id": "Zq9HwRh47gtT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xL3hsGMOmodH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}