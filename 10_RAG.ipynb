{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyadeep-Dey/AI-experiments/blob/main/10_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG with fallback to LLM\n",
        "### First use RAG to answer questions from a knowledge base stored in Chroma Vector DB\n",
        "For this we use ConversationalRetrievalChain\n",
        "###Then we'll see how to get data from LLM directly in case RAG cannot answer the question\n",
        "Direct retriever is used in this scenario"
      ],
      "metadata": {
        "id": "Vsvg7eXTc81n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-chroma langchain-core openai tiktoken chromadb transformers sentence-transformers langchain-community\n"
      ],
      "metadata": {
        "id": "WtUVraR_XgfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import login\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import Document\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "# needed for falling back to LLM when KB does not have required info\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "Yip5mfo-aiN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function : Read from a file"
      ],
      "metadata": {
        "id": "Ba0zTQuFShtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_from_file(folder_path, file_name):\n",
        "\n",
        "  # Always mount Drive explicitly when using Google Drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  print(\"Drive mounted.\")\n",
        "\n",
        "  # Wait until MyDrive is available\n",
        "  mydrive_path = '/content/drive/MyDrive'\n",
        "  while not os.path.exists(mydrive_path):\n",
        "      print(\"Waiting for Drive to be ready...\")\n",
        "      time.sleep(1)\n",
        "\n",
        "  # Path to the file\n",
        "  file_path = os.path.join(mydrive_path, folder_path, file_name)\n",
        "\n",
        "  # Check if the file exists\n",
        "  if os.path.exists(file_path):\n",
        "      # Read the content of the file\n",
        "      with open(file_path, 'r') as file:\n",
        "          contents = file.read()\n",
        "      return contents\n",
        "  else:\n",
        "      return \"File not found!\"\n"
      ],
      "metadata": {
        "id": "lxoY2_L3OOdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "GPT_4o_mini = \"gpt-4o-mini\"\n",
        "GPT_4o =\"gpt-4o\"\n",
        "db_name = \"vector_db\"\n"
      ],
      "metadata": {
        "id": "GI6IOTwXP9Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "oMk5VfGxYB6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI using Secrets in Colab\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)\n"
      ],
      "metadata": {
        "id": "VgVt8guSYGvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read the text first\n",
        "\n",
        "original_content = read_text_from_file(\n",
        "    folder_path=\"Files/Knowledge-Base\",\n",
        "    file_name= \"Anonymized by OpenAI_TOTC.txt\" #\"Anonymized by OpenAI_TOTC_V4.txt\"\n",
        ")\n",
        "\n",
        "print(f\"The number of characters are : {len(original_content)}\")\n",
        "number_of_words = len(original_content.split())\n",
        "# Divides a string into a list of substrings based on a specified separator (default is whitespace) and then counts length of list\n",
        "print(f\"Number of words is : {number_of_words}\")\n",
        "# print()\n",
        "# print(original_content)\n",
        "\n"
      ],
      "metadata": {
        "id": "VUE7MQ4vDRYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap text as a Document\n",
        "doc = Document(page_content=original_content)\n",
        "\n",
        "# Split into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents([doc])\n",
        "print(f\"Total number of chunks: {len(chunks)}\")\n"
      ],
      "metadata": {
        "id": "xL3hsGMOmodH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "# Note : we need to use 'openai_api_key=openai_api_key' because we're using LangChain and not Open AI directly !\n",
        "\n",
        "# Create vectorstore\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
        "# Note : The vector DB will be stored in the local folder of this Notebook and will be lost when we disconnect from runtime\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "id": "M6uoTqUsFwyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new Chat with OpenAI\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key,temperature=0.7, model_name=GPT_4o_mini)\n",
        "\n",
        "# set up the conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "id": "QxwXR6ntIpNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a simple question\n",
        "\n",
        "query = \"Who is the author of A Chronicle of Two Cities\"\n",
        "result = conversation_chain.invoke({\"question\": query})\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "id": "2ytIejQ-JHli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a better way to represent this\n",
        "chat_history_data = result\n",
        "\n",
        "# Iterate over the messages and print who said what\n",
        "for message in chat_history_data['chat_history']:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"AI: {message.content}\")"
      ],
      "metadata": {
        "id": "VO14QBTNRRh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.clear()\n",
        "# since we want to ask another un-related question\n",
        "\n",
        "query = \"Can you describe Elise Manet in a few sentences\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"]) # just answer .. not entire chat\n",
        "\n",
        "query = \"Who is Elise Manet married to ?\" # chat retains context and so knows that \"his\" means Alex .\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])\n",
        "\n",
        "#Uncomment print statement to see complete chat history .You'll see previous question and answer are part of chat_history\n",
        "#print(chat_history_data)"
      ],
      "metadata": {
        "id": "PMCUrFE1JcQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is Philippe Duval married to ?\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])\n",
        "\n",
        "query = \"What is his original name ?\" # chat retains context and so knows what we mean by \"his\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])\n"
      ],
      "metadata": {
        "id": "c6mzHY5dKUL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who are the owners of the wine shop in this story ?\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])"
      ],
      "metadata": {
        "id": "iWET8OqcKqK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is on trial for treason against England ?\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])"
      ],
      "metadata": {
        "id": "zmqhbwxwLXOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why is he on trial ?\" # chat retains context and so knows what we mean by \"he\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])\n",
        "\n",
        "query = \"Who are the witnesses in this trial ?\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])"
      ],
      "metadata": {
        "id": "Aa3AYkoULjOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the capital of India ?\"\n",
        "chat_history_data = conversation_chain.invoke({\"question\":query})\n",
        "print(chat_history_data[\"answer\"])\n",
        "# cannot answer because it's only looking at KB in vector DB"
      ],
      "metadata": {
        "id": "Hj5iMA75MLKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a decision chain to route between RAG and direct LLM\n",
        "\n",
        "*    Using Direct retriever ... NO conversation history\n",
        "*    This code is courtesy Claude 3.7 Sonnet. Works w/o any change !!\n",
        "*    First tried with Open AI ChatGPT but didn't work !"
      ],
      "metadata": {
        "id": "64cYOPqOb0fG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bET38OQ4c63O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have llm, retriever, and memory already set up\n",
        "\n",
        "# Create a decision chain to route between RAG and direct LLM\n",
        "decision_template = \"\"\"Determine if the following question requires domain-specific knowledge or is general knowledge.\n",
        "If the question is about a book titled 'A Chronicle of Two Cities', it's characters , events , places or specialized information\n",
        "likely in your knowledge base, respond with \"DOMAIN\".\n",
        "If the question is about general facts like capitals, history, science, or common knowledge, respond with \"GENERAL\".\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Decision (DOMAIN/GENERAL):\"\"\"\n",
        "\n",
        "decision_prompt = PromptTemplate(template=decision_template, input_variables=[\"question\"])\n",
        "decision_chain = LLMChain(llm=llm, prompt=decision_prompt)\n",
        "\n",
        "# Create a simple function to handle the routing logic\n",
        "def smart_qa(question):\n",
        "    # First determine if we should use RAG or direct LLM\n",
        "    decision_result = decision_chain.run(question=question)\n",
        "    print(f\"Decision: {decision_result}\")\n",
        "\n",
        "    if \"DOMAIN\" in decision_result.upper():\n",
        "        # Use RAG for domain-specific questions\n",
        "        docs = retriever.get_relevant_documents(question)\n",
        "        #print(docs)\n",
        "        if docs:\n",
        "            # Create a QA chain for RAG\n",
        "            qa_chain = load_qa_chain(llm=llm, chain_type=\"stuff\")\n",
        "            return qa_chain.run(input_documents=docs, question=question)\n",
        "        else:\n",
        "            # Fall back to general if no docs found\n",
        "            return llm.generate([[HumanMessage(content=f\"Answer this question: {question}\")]])\\\n",
        "                  .generations[0][0].text\n",
        "    else:\n",
        "        # Use direct LLM for general knowledge questions\n",
        "        return llm.generate([[HumanMessage(content=f\"Answer this general knowledge question: {question}\")]])\\\n",
        "              .generations[0][0].text\n"
      ],
      "metadata": {
        "id": "CWTn6E3oUuPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "print(smart_qa(\"What is the capital of Australia?\")) # GENERAL\n",
        "print()\n",
        "print(smart_qa(\"Who is the author of the book A Chronicle of Two Cities\")) #DOMAIN\n",
        "#above question creates a problem for the LLM . It classifies it as GENERAL\n",
        "# probably because name of the book is similar to the original.\n",
        "print()\n",
        "print(smart_qa(\"Who are the owners of the wine shop in this story ?\")) #DOMAIN\n",
        "print()\n",
        "print(smart_qa(\"Can you describe Elise Manet in a few sentences\")) #DOMAIN\n",
        "print()\n",
        "print(smart_qa(\"Who is the Jackal and who is the Lion ?\"))#DOMAIN\n",
        "print()\n",
        "print(smart_qa(\"Who is Philippe Duval married to ?\"))#DOMAIN\n",
        "print()\n",
        "print(smart_qa(\"What is his original name ?\"))#DOMAIN\n",
        "print()\n",
        "print(smart_qa(\"Who are the witnesses in this trial ?\")) #DOMAIN\n",
        "print()\n",
        "print(smart_qa(\"What is the capital of UK ?\")) #GENERAL\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "id": "2Fy5jIE8VMsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nDL8o8WN_x1N"
      }
    }
  ]
}